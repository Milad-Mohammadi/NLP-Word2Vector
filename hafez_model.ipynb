{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hafez_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcess"
      ],
      "metadata": {
        "id": "o38FIWrODusU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read DataSet"
      ],
      "metadata": {
        "id": "XB9C4rk729vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RJxdvrESCjDl"
      },
      "outputs": [],
      "source": [
        "with open (\"hafez.txt\", \"r\") as dataset:\n",
        "    data = dataset.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rLf8MSKCFzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Stop Words"
      ],
      "metadata": {
        "id": "o9M1DgCg3MDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"fa_stop_words.txt\", \"r\") as fa_stop_words:\n",
        "    stop_words = fa_stop_words.read().splitlines()"
      ],
      "metadata": {
        "id": "WOcwLHXTF1iW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "R4aF4YtVGD-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize with Hazm"
      ],
      "metadata": {
        "id": "tuy4eEFz3QaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "0_UPQvI83TmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "stJGO03q5zjF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "normalized_data = []\n",
        "for line in data:\n",
        "  normalized_data.append(normalizer.normalize(line))"
      ],
      "metadata": {
        "id": "fZ0BK1EY53xt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_data"
      ],
      "metadata": {
        "id": "w5GE9B919UQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize with Hazm"
      ],
      "metadata": {
        "id": "nnhz9adK9yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = []\n",
        "for line in normalized_data:\n",
        "  tokenized_data.append(word_tokenize(line))"
      ],
      "metadata": {
        "id": "-jsyndnn92KB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data"
      ],
      "metadata": {
        "id": "WDQ17gep-z_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Stop Words"
      ],
      "metadata": {
        "id": "lSkGm9cTDnra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data = []\n",
        "for line in tokenized_data:\n",
        "  clear_line = []\n",
        "  for word in line:\n",
        "    if word not in stop_words:\n",
        "      clear_line.append(word)\n",
        "  clear_data.append(clear_line)"
      ],
      "metadata": {
        "id": "onhCuzYbDsH3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data"
      ],
      "metadata": {
        "id": "FC8TkstXEom7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming with Hazm\n",
        "    Finding the root of words"
      ],
      "metadata": {
        "id": "W_eLtIioMKEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer()\n",
        "stemmed_data = []\n",
        "for line in clear_data:\n",
        "  stemmed_line = []\n",
        "  for word in line:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if len(stemmed_word) > 1:\n",
        "      stemmed_line.append(stemmed_word)\n",
        "\n",
        "  stemmed_data.append(stemmed_line)  "
      ],
      "metadata": {
        "id": "dXcMAOu4MOVH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_data"
      ],
      "metadata": {
        "id": "z-t167UiSwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert line arrays to sentence"
      ],
      "metadata": {
        "id": "Pq-l1ierYunJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences = []\n",
        "for line in stemmed_data:\n",
        "  clear_sentences.append(\" \".join(line))\n",
        "\n",
        "clear_sentences = [item for item in clear_sentences if item]  "
      ],
      "metadata": {
        "id": "54N3_P9gY0JE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences"
      ],
      "metadata": {
        "id": "7kPp1UtlZH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process"
      ],
      "metadata": {
        "id": "xn_5QzMlhgeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the corpus vocabulary"
      ],
      "metadata": {
        "id": "mVqELWb1jdVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(clear_sentences)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1 \n",
        "embed_size = 100\n",
        "\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in clear_sentences]\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "metadata": {
        "id": "iRHoozbUhwER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a skip-gram generator"
      ],
      "metadata": {
        "id": "VgLOTYZtjiwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "#Remove empty elements\n",
        "for i, element in enumerate(skip_grams):\n",
        "  skip_grams[i] = [item for item in element if item]  \n",
        "skip_grams = [item for item in skip_grams if item]  \n",
        "\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))\n"
      ],
      "metadata": {
        "id": "luP3a231joqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the skip-gram model architecture"
      ],
      "metadata": {
        "id": "qS7F23QtmrCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "merged_output = add([word_model.output, context_model.output]) \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "final_model = Model([word_model.input, context_model.input], model(merged_output))\n",
        "final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "final_model.summary()\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(final_model, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg')) "
      ],
      "metadata": {
        "id": "sJCdvgHPmtUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "mT322OPIS0yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skip_grams"
      ],
      "metadata": {
        "id": "e7tSfb3NlgQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "     loss = 0\n",
        "     for i, element in enumerate(skip_grams):\n",
        "         pair_first_element = np.array(list(zip(*element[0]))[0], dtype='int32')\n",
        "         pair_second_element = np.array(list(zip(*element[0]))[1], dtype='int32')\n",
        "         labels = np.array(element[1], dtype='int32')\n",
        "         X = [pair_first_element, pair_second_element]\n",
        "         Y = labels\n",
        "         if i % 10000 == 0:\n",
        "             print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "         loss += final_model.train_on_batch(X,Y)  \n",
        "     print('Epoch:', epoch, 'Loss:', loss) "
      ],
      "metadata": {
        "id": "LbFgdK2wS4Fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4608a4-b71b-4861-fe27-64dddf03279b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 1621.2176669342443\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 1434.3298737863079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get word embeddings"
      ],
      "metadata": {
        "id": "d6ulcz9StxZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "word_embed_layer = word_model.layers[0]\n",
        "weights = word_embed_layer.get_weights()[0][1:]\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] for search_term in ['حافظ', 'خط','دل', 'عشق', 'خدا']}\n",
        "similar_words "
      ],
      "metadata": {
        "id": "oRBRX5k6t07z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067689fa-3f07-45b3-d39f-e6b9fcc3406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5690, 5690)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'حافظ': ['استظهار', 'رخنه', 'کشفته', 'بخواند', 'مونس'],\n",
              " 'خدا': ['کلید', 'کاکل', 'جفاک', 'شاهراه', 'بارد'],\n",
              " 'خط': ['ناله', 'هوادار', 'عال', 'نمی\\u200cآید', 'میان'],\n",
              " 'دل': ['ندیده', 'چشم', 'بخیل', 'بیزار', 'بازآ'],\n",
              " 'عشق': ['وآنچه', 'صب', 'خطرهاس', 'مهرگیاه', 'همی\\u200cبند']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}