{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hafez_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcess"
      ],
      "metadata": {
        "id": "o38FIWrODusU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read DataSet"
      ],
      "metadata": {
        "id": "XB9C4rk729vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RJxdvrESCjDl"
      },
      "outputs": [],
      "source": [
        "with open (\"hafez.txt\", \"r\") as dataset:\n",
        "    data = dataset.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rLf8MSKCFzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Stop Words"
      ],
      "metadata": {
        "id": "o9M1DgCg3MDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"fa_stop_words.txt\", \"r\") as fa_stop_words:\n",
        "    stop_words = fa_stop_words.read().splitlines()"
      ],
      "metadata": {
        "id": "WOcwLHXTF1iW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "R4aF4YtVGD-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize with Hazm"
      ],
      "metadata": {
        "id": "tuy4eEFz3QaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hazm"
      ],
      "metadata": {
        "id": "0_UPQvI83TmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "stJGO03q5zjF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "normalized_data = []\n",
        "for line in data:\n",
        "  normalized_data.append(normalizer.normalize(line))"
      ],
      "metadata": {
        "id": "fZ0BK1EY53xt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_data"
      ],
      "metadata": {
        "id": "w5GE9B919UQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize with Hazm"
      ],
      "metadata": {
        "id": "nnhz9adK9yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = []\n",
        "for line in normalized_data:\n",
        "  tokenized_data.append(word_tokenize(line))"
      ],
      "metadata": {
        "id": "-jsyndnn92KB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data"
      ],
      "metadata": {
        "id": "WDQ17gep-z_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Stop Words"
      ],
      "metadata": {
        "id": "lSkGm9cTDnra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data = []\n",
        "for line in tokenized_data:\n",
        "  clear_line = []\n",
        "  for word in line:\n",
        "    if word not in stop_words:\n",
        "      clear_line.append(word)\n",
        "  clear_data.append(clear_line)"
      ],
      "metadata": {
        "id": "onhCuzYbDsH3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data"
      ],
      "metadata": {
        "id": "FC8TkstXEom7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming with Hazm\n",
        "    Finding the root of words"
      ],
      "metadata": {
        "id": "W_eLtIioMKEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer()\n",
        "stemmed_data = []\n",
        "for line in clear_data:\n",
        "  stemmed_line = []\n",
        "  for word in line:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if len(stemmed_word) > 1:\n",
        "      stemmed_line.append(stemmed_word)\n",
        "\n",
        "  stemmed_data.append(stemmed_line)  "
      ],
      "metadata": {
        "id": "dXcMAOu4MOVH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_data"
      ],
      "metadata": {
        "id": "z-t167UiSwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert line arrays to sentence"
      ],
      "metadata": {
        "id": "Pq-l1ierYunJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences = []\n",
        "for line in stemmed_data:\n",
        "  clear_sentences.append(\" \".join(line))"
      ],
      "metadata": {
        "id": "54N3_P9gY0JE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences"
      ],
      "metadata": {
        "id": "7kPp1UtlZH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process"
      ],
      "metadata": {
        "id": "xn_5QzMlhgeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "GXagZ9ljhf5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(clear_sentences)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1 \n",
        "embed_size = 100\n",
        "\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in clear_sentences]\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "metadata": {
        "id": "iRHoozbUhwER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}