{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hafez_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcess"
      ],
      "metadata": {
        "id": "o38FIWrODusU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read DataSet"
      ],
      "metadata": {
        "id": "XB9C4rk729vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RJxdvrESCjDl"
      },
      "outputs": [],
      "source": [
        "with open (\"hafez.txt\", \"r\") as dataset:\n",
        "    data = dataset.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rLf8MSKCFzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Stop Words"
      ],
      "metadata": {
        "id": "o9M1DgCg3MDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"fa_stop_words.txt\", \"r\") as fa_stop_words:\n",
        "    stop_words = fa_stop_words.read().splitlines()"
      ],
      "metadata": {
        "id": "WOcwLHXTF1iW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "R4aF4YtVGD-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize with Hazm"
      ],
      "metadata": {
        "id": "tuy4eEFz3QaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "0_UPQvI83TmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e08ed6-8a33-4557-bda9-0db6438655a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 316 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 61.5 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=e5c5eb666d05eb3702840ed250299ed23c078b48dd6354e907ac9070ecbe26b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154467 sha256=82e6cb9cac00698d7652aa17d640c57a5fbf57c2fb2e7bc1fc2a47d2bef48ce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "stJGO03q5zjF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "normalized_data = []\n",
        "for line in data:\n",
        "  normalized_data.append(normalizer.normalize(line))"
      ],
      "metadata": {
        "id": "fZ0BK1EY53xt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_data"
      ],
      "metadata": {
        "id": "w5GE9B919UQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize with Hazm"
      ],
      "metadata": {
        "id": "nnhz9adK9yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = []\n",
        "for line in normalized_data:\n",
        "  tokenized_data.append(word_tokenize(line))"
      ],
      "metadata": {
        "id": "-jsyndnn92KB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data"
      ],
      "metadata": {
        "id": "WDQ17gep-z_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Stop Words"
      ],
      "metadata": {
        "id": "lSkGm9cTDnra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data = []\n",
        "for line in tokenized_data:\n",
        "  clear_line = []\n",
        "  for word in line:\n",
        "    if word not in stop_words:\n",
        "      clear_line.append(word)\n",
        "  clear_data.append(clear_line)"
      ],
      "metadata": {
        "id": "onhCuzYbDsH3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data"
      ],
      "metadata": {
        "id": "FC8TkstXEom7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming with Hazm\n",
        "    Finding the root of words"
      ],
      "metadata": {
        "id": "W_eLtIioMKEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer()\n",
        "stemmed_data = []\n",
        "for line in clear_data:\n",
        "  stemmed_line = []\n",
        "  for word in line:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if len(stemmed_word) > 1:\n",
        "      stemmed_line.append(stemmed_word)\n",
        "\n",
        "  stemmed_data.append(stemmed_line)  "
      ],
      "metadata": {
        "id": "dXcMAOu4MOVH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_data"
      ],
      "metadata": {
        "id": "z-t167UiSwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert line arrays to sentence"
      ],
      "metadata": {
        "id": "Pq-l1ierYunJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences = []\n",
        "for line in stemmed_data:\n",
        "  clear_sentences.append(\" \".join(line))\n",
        "\n",
        "clear_sentences = [item for item in clear_sentences if item]  "
      ],
      "metadata": {
        "id": "54N3_P9gY0JE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_sentences"
      ],
      "metadata": {
        "id": "7kPp1UtlZH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process"
      ],
      "metadata": {
        "id": "xn_5QzMlhgeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the corpus vocabulary"
      ],
      "metadata": {
        "id": "mVqELWb1jdVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(clear_sentences)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1 \n",
        "embed_size = 100\n",
        "\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in clear_sentences]\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "metadata": {
        "id": "iRHoozbUhwER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a skip-gram generator"
      ],
      "metadata": {
        "id": "VgLOTYZtjiwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))\n"
      ],
      "metadata": {
        "id": "luP3a231joqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the skip-gram model architecture"
      ],
      "metadata": {
        "id": "qS7F23QtmrCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "merged_output = add([word_model.output, context_model.output]) \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "final_model = Model([word_model.input, context_model.input], model(merged_output))\n",
        "final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "final_model.summary()\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(final_model, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg')) "
      ],
      "metadata": {
        "id": "sJCdvgHPmtUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}