{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38FIWrODusU"
      },
      "source": [
        "# PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RJxdvrESCjDl"
      },
      "outputs": [],
      "source": [
        "#Read DataSet and Stop words\n",
        "\n",
        "with open (\"hafez.txt\", \"r\") as dataset:\n",
        "    data = dataset.read().splitlines()\n",
        "\n",
        "with open (\"fa_stop_words.txt\", \"r\") as fa_stop_words:\n",
        "    stop_words = fa_stop_words.read().splitlines()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rLf8MSKCFzS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753cdbd7-979b-455d-fe2e-836e37a55473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: \n",
            "1:  ﻿ \n",
            "2:  الا يا ايها الساقي ادر كاسا و ناولها \n",
            "3:  كه عشق آسان نمود اول ولي افتاد مشكل‌ها\n",
            "\n",
            "stop_words: \n",
            "1:  ؟ \n",
            "2:  آباد \n",
            "3:  آخ\n"
          ]
        }
      ],
      "source": [
        "print(\"data:\", \"\\n1: \", data[0], \"\\n2: \", data[1], \"\\n3: \", data[2])\n",
        "print(\"\\nstop_words:\", \"\\n1: \", stop_words[18], \"\\n2: \", stop_words[19], \"\\n3: \", stop_words[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-H3zQowC4X9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_UPQvI83TmK"
      },
      "outputs": [],
      "source": [
        "!pip install hazm\n",
        "\n",
        "from hazm import *\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fZ0BK1EY53xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331d97c9-b296-4620-b4c6-f44015d88bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized:  الا یا ایها الساقی ادر کاسا و ناولها\n",
            "Tokenized:  ['الا', 'یا', 'ایها', 'الساقی', 'ادر', 'کاسا', 'و', 'ناولها']\n",
            "Cleared:  ['ایها', 'الساقی', 'ادر', 'کاسا', 'ناولها']\n",
            "Words:  ایها size:  34482\n",
            "Non Duplicate Words:  ایها size:  8168\n",
            "Sentence:  ایها الساقی ادر کاسا ناولها\n"
          ]
        }
      ],
      "source": [
        "normalizer = Normalizer()\n",
        "normalized_data = []\n",
        "tokenized_data = []\n",
        "clear_data = []\n",
        "clear_sentences = []\n",
        "words = []\n",
        "clear_words = []\n",
        "\n",
        "#### Normalize\n",
        "for line in data:\n",
        "  if line and line != '\\ufeff':\n",
        "    normalized_data.append(normalizer.normalize(line))\n",
        "\n",
        "\n",
        "#### Tokenize\n",
        "for line in normalized_data:\n",
        "  tokenized_data.append(word_tokenize(line))\n",
        "\n",
        "\n",
        "#### Remove stop words\n",
        "for line in tokenized_data:\n",
        "  clear_line = []\n",
        "  for word in line:\n",
        "    if word not in stop_words:\n",
        "      clear_line.append(word)\n",
        "  clear_data.append(clear_line)\n",
        "\n",
        "\n",
        "#### Create Bag of words\n",
        "for i in range(len(clear_data)):\n",
        "  words.extend(clear_data[i])\n",
        "\n",
        "\n",
        "#### Convert words to sentences\n",
        "for line in clear_data:\n",
        "  clear_sentences.append(\" \".join(line))\n",
        "clear_sentences = [item for item in clear_sentences if item]\n",
        "\n",
        "\n",
        "# Remove duplicate words\n",
        "for i in words: \n",
        "  if i not in clear_words: \n",
        "    clear_words.append(i) \n",
        "\n",
        "\n",
        "print(\"Normalized: \", normalized_data[0])\n",
        "print(\"Tokenized: \", tokenized_data[0])\n",
        "print(\"Cleared: \", clear_data[0])\n",
        "print(\"Words: \", words[0], \"size: \", len(words))\n",
        "print(\"Non Duplicate Words: \", clear_words[0], \"size: \", len(clear_words))\n",
        "print(\"Sentence: \", clear_sentences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QxGIOjErbC"
      },
      "source": [
        "# Process"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hafez_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}