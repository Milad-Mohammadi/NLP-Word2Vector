{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38FIWrODusU"
      },
      "source": [
        "# PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RJxdvrESCjDl"
      },
      "outputs": [],
      "source": [
        "#Read DataSet and Stop words\n",
        "\n",
        "with open (\"hafez.txt\", \"r\") as dataset:\n",
        "    data = dataset.read().splitlines()\n",
        "\n",
        "with open (\"fa_stop_words.txt\", \"r\") as fa_stop_words:\n",
        "    stop_words = fa_stop_words.read().splitlines()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLf8MSKCFzS6",
        "outputId": "fa4a76fc-5872-4779-a5b2-cc0d2e6b2732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: \n",
            "1:  ﻿ \n",
            "2:  الا يا ايها الساقي ادر كاسا و ناولها \n",
            "3:  كه عشق آسان نمود اول ولي افتاد مشكل‌ها\n",
            "\n",
            "stop_words: \n",
            "1:  ؟ \n",
            "2:  آباد \n",
            "3:  آخ\n"
          ]
        }
      ],
      "source": [
        "print(\"data:\", \"\\n1: \", data[0], \"\\n2: \", data[1], \"\\n3: \", data[2])\n",
        "print(\"\\nstop_words:\", \"\\n1: \", stop_words[18], \"\\n2: \", stop_words[19], \"\\n3: \", stop_words[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H3zQowC4X9A"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0_UPQvI83TmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfa6b33-62eb-473a-b32e-51aa28548864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "\n",
        "from hazm import *\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ0BK1EY53xt",
        "outputId": "2865225d-1d23-4bbc-90e3-c27fc68322b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized:  الا یا ایها الساقی ادر کاسا و ناولها\n",
            "Tokenized:  ['الا', 'یا', 'ایها', 'الساقی', 'ادر', 'کاسا', 'و', 'ناولها']\n",
            "Cleared:  ['ایها', 'الساقی', 'ادر', 'کاسا', 'ناولها']\n",
            "Words:  ای size:  34482\n",
            "Non Duplicate Words:  ای size:  5773\n",
            "Sentence:  ای الساق ادر کاسا ناول\n"
          ]
        }
      ],
      "source": [
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "normalized_data = []\n",
        "tokenized_data = []\n",
        "clear_data = []\n",
        "stemmed_data = []\n",
        "clear_sentences = []\n",
        "words = []\n",
        "clear_words = []\n",
        "\n",
        "#### Normalize\n",
        "for line in data:\n",
        "  if line and line != '\\ufeff':\n",
        "    normalized_data.append(normalizer.normalize(line))\n",
        "\n",
        "\n",
        "#### Tokenize\n",
        "for line in normalized_data:\n",
        "  tokenized_data.append(word_tokenize(line))\n",
        "\n",
        "\n",
        "#### Remove stop words\n",
        "for line in tokenized_data:\n",
        "  clear_line = []\n",
        "  for word in line:\n",
        "    if word not in stop_words:\n",
        "      clear_line.append(word)\n",
        "  clear_data.append(clear_line)\n",
        "\n",
        "\n",
        "\n",
        "#### Stemize\n",
        "for line in clear_data:\n",
        "  stemmed_line = []\n",
        "  for word in line:\n",
        "    stemmed_line.append(stemmer.stem(word))\n",
        "  stemmed_data.append(stemmed_line)\n",
        "\n",
        "\n",
        "#### Create Bag of words\n",
        "for i in range(len(stemmed_data)):\n",
        "  words.extend(stemmed_data[i])\n",
        "\n",
        "\n",
        "#### Convert words to sentences\n",
        "for line in stemmed_data:\n",
        "  clear_sentences.append(\" \".join(line))\n",
        "clear_sentences = [item for item in clear_sentences if item]\n",
        "\n",
        "\n",
        "# Remove duplicate words\n",
        "for i in words: \n",
        "  if i not in clear_words: \n",
        "    clear_words.append(i) \n",
        "\n",
        "\n",
        "print(\"Normalized: \", normalized_data[0])\n",
        "print(\"Tokenized: \", tokenized_data[0])\n",
        "print(\"Cleared: \", clear_data[0])\n",
        "print(\"Words: \", words[0], \"size: \", len(words))\n",
        "print(\"Non Duplicate Words: \", clear_words[0], \"size: \", len(clear_words))\n",
        "print(\"Sentence: \", clear_sentences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QxGIOjErbC"
      },
      "source": [
        "# Process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "SEED = 42\n",
        "NUM_NS = 4\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "04qu2wE1vtZM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = len(clear_words)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(clear_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(clear_sentences)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = { v:k for k, v in word2id.items() }\n",
        "\n",
        "max_len = 20\n",
        "inverse_vocab = pad_sequences(sequences, padding = 'pre', maxlen= max_len)\n",
        "print(inverse_vocab)"
      ],
      "metadata": {
        "id": "9het9yBQvTs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1686c1eb-a065-4ef2-ba13-bbb004e23bf0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0 ... 2998 2143 2999]\n",
            " [   0    0    0 ...  591  152  389]\n",
            " [   0    0    0 ...   41  217  952]\n",
            " ...\n",
            " [   0    0    0 ...    0   29  103]\n",
            " [   0    0    0 ... 2791  451   45]\n",
            " [   0    0    0 ...   96 5713    2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sequences, window_size, vocab_size):\n",
        "  targets, contexts, labels = [], [], []\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=NUM_NS,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "      \n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*NUM_NS, dtype=\"int64\")\n",
        "\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "metadata": {
        "id": "Z4v8tu8S3are"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(clear_words)\n",
        "\n",
        "\n",
        "targets, contexts, labels = generate_training_data(sequences=sequences, window_size= 2, vocab_size=vocab_size)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)[:,:,0]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksxw0-Lr2aAp",
        "outputId": "87b573bc-6f9e-44f4-81b7-1da275d903dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8384/8384 [00:04<00:00, 1819.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (17216,)\n",
            "contexts.shape: (17216, 5)\n",
            "labels.shape: (17216, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KDxRF9f5Pqs",
        "outputId": "aff57152-7c2b-407b-ced3-8f11a896b03b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ht_4_VK5Uir",
        "outputId": "45b9e9fb-679a-42a0-d40f-c5a52f166efd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size, embedding_dim, input_length=NUM_NS+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    \n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "\n",
        "    return dots"
      ],
      "metadata": {
        "id": "bGdpnnx95bUX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "metadata": {
        "id": "oWFFZnct5fM3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "J7gYmQYt5iis"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "xttawI2L5liF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=80, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "4XeUY86l5oOS",
        "outputId": "8dd4f4c6-c5d7-46a2-8f1e-fa29465278a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 1.6093 - accuracy: 0.2052\n",
            "Epoch 2/80\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 1.5968 - accuracy: 0.6589\n",
            "Epoch 3/80\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 1.5847 - accuracy: 0.8782\n",
            "Epoch 4/80\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 1.5697 - accuracy: 0.9433\n",
            "Epoch 5/80\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 1.5503 - accuracy: 0.9665\n",
            "Epoch 6/80\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 1.5245 - accuracy: 0.9744\n",
            "Epoch 7/80\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.4910 - accuracy: 0.9773\n",
            "Epoch 8/80\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 1.4485 - accuracy: 0.9781\n",
            "Epoch 9/80\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 1.3965 - accuracy: 0.9785\n",
            "Epoch 10/80\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 1.3351 - accuracy: 0.9784\n",
            "Epoch 11/80\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 1.2653 - accuracy: 0.9780\n",
            "Epoch 12/80\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 1.1885 - accuracy: 0.9780\n",
            "Epoch 13/80\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 1.1068 - accuracy: 0.9783\n",
            "Epoch 14/80\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 1.0222 - accuracy: 0.9788\n",
            "Epoch 15/80\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.9369 - accuracy: 0.9796\n",
            "Epoch 16/80\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.8529 - accuracy: 0.9806\n",
            "Epoch 17/80\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.7720 - accuracy: 0.9821\n",
            "Epoch 18/80\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.6955 - accuracy: 0.9839\n",
            "Epoch 19/80\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.6244 - accuracy: 0.9854\n",
            "Epoch 20/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.5592 - accuracy: 0.9861\n",
            "Epoch 21/80\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.5002 - accuracy: 0.9878\n",
            "Epoch 22/80\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.4474 - accuracy: 0.9894\n",
            "Epoch 23/80\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.4005 - accuracy: 0.9911\n",
            "Epoch 24/80\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.3589 - accuracy: 0.9927\n",
            "Epoch 25/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.3224 - accuracy: 0.9938\n",
            "Epoch 26/80\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.2903 - accuracy: 0.9948\n",
            "Epoch 27/80\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.2622 - accuracy: 0.9960\n",
            "Epoch 28/80\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.2375 - accuracy: 0.9966\n",
            "Epoch 29/80\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.2158 - accuracy: 0.9978\n",
            "Epoch 30/80\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.1967 - accuracy: 0.9981\n",
            "Epoch 31/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.1800 - accuracy: 0.9987\n",
            "Epoch 32/80\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1651 - accuracy: 0.9989\n",
            "Epoch 33/80\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1520 - accuracy: 0.9989\n",
            "Epoch 34/80\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.1404 - accuracy: 0.9990\n",
            "Epoch 35/80\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1300 - accuracy: 0.9992\n",
            "Epoch 36/80\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1208 - accuracy: 0.9994\n",
            "Epoch 37/80\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.1125 - accuracy: 0.9995\n",
            "Epoch 38/80\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.1050 - accuracy: 0.9996\n",
            "Epoch 39/80\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0983 - accuracy: 0.9996\n",
            "Epoch 40/80\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0923 - accuracy: 0.9996\n",
            "Epoch 41/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0868 - accuracy: 0.9996\n",
            "Epoch 42/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0819 - accuracy: 0.9996\n",
            "Epoch 43/80\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0773 - accuracy: 0.9996\n",
            "Epoch 44/80\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0732 - accuracy: 0.9996\n",
            "Epoch 45/80\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0694 - accuracy: 0.9996\n",
            "Epoch 46/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0660 - accuracy: 0.9996\n",
            "Epoch 47/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0628 - accuracy: 0.9996\n",
            "Epoch 48/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0598 - accuracy: 0.9996\n",
            "Epoch 49/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0571 - accuracy: 0.9996\n",
            "Epoch 50/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0546 - accuracy: 0.9996\n",
            "Epoch 51/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0523 - accuracy: 0.9996\n",
            "Epoch 52/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0502 - accuracy: 0.9996\n",
            "Epoch 53/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0482 - accuracy: 0.9996\n",
            "Epoch 54/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0463 - accuracy: 0.9996\n",
            "Epoch 55/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0446 - accuracy: 0.9996\n",
            "Epoch 56/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.9996\n",
            "Epoch 57/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0414 - accuracy: 0.9996\n",
            "Epoch 58/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0400 - accuracy: 0.9996\n",
            "Epoch 59/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0387 - accuracy: 0.9996\n",
            "Epoch 60/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0375 - accuracy: 0.9997\n",
            "Epoch 61/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0363 - accuracy: 0.9997\n",
            "Epoch 62/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0352 - accuracy: 0.9997\n",
            "Epoch 63/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0341 - accuracy: 0.9997\n",
            "Epoch 64/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0332 - accuracy: 0.9997\n",
            "Epoch 65/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0322 - accuracy: 0.9997\n",
            "Epoch 66/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0313 - accuracy: 0.9997\n",
            "Epoch 67/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0305 - accuracy: 0.9997\n",
            "Epoch 68/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0297 - accuracy: 0.9997\n",
            "Epoch 69/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0290 - accuracy: 0.9997\n",
            "Epoch 70/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0283 - accuracy: 0.9997\n",
            "Epoch 71/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0276 - accuracy: 0.9997\n",
            "Epoch 72/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0270 - accuracy: 0.9997\n",
            "Epoch 73/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0264 - accuracy: 0.9997\n",
            "Epoch 74/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0258 - accuracy: 0.9997\n",
            "Epoch 75/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0252 - accuracy: 0.9997\n",
            "Epoch 76/80\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0247 - accuracy: 0.9997\n",
            "Epoch 77/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0242 - accuracy: 0.9997\n",
            "Epoch 78/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0237 - accuracy: 0.9997\n",
            "Epoch 79/80\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0233 - accuracy: 0.9997\n",
            "Epoch 80/80\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0228 - accuracy: 0.9997\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0251873790>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "\n",
        "similar_words = {  search_term:  [ id2word[idx] for idx in distance_matrix[word2id[search_term]].argsort()[1:6]] for search_term in ['عشق', 'حافظ', 'می', 'جام', 'دیوانه'] }\n",
        "\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9fiLvOaL1I-",
        "outputId": "2af8400f-4d0e-4fa8-f2da-a229c35f0ea3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'جام': ['زمن', 'شکس', 'حضوری\\u200cگر', 'مناز', 'دوخته'],\n",
              " 'حافظ': ['می\\u200cسوز', 'ننگرد', 'العین', 'بیارا', 'مقدر'],\n",
              " 'دیوانه': ['لاابال', 'آشناس', 'گزارند', 'ننگرد', 'توشه'],\n",
              " 'عشق': ['پاکباز', 'رمیدن', 'بنمود', 'معماییس', 'فرهادک'],\n",
              " 'می': ['دیرگاه', 'گهربار', 'شرمسار', 'جاندار', 'خاتم']}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.save('hafez')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POLmBWtWVM4m",
        "outputId": "4b18df9b-29c8-473d-9d5d-3a6f9ff9f3c0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: hafez/assets\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hafez_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}